{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Codes  \n",
    "BABS_V 506 BA1 BA2 2024W1 Analyzing and Modelling Uncertainty = 147283  \n",
    "BABS_V 507 BA1 BA2 2024W1 Descriptive and Predictive Business Analysis = 145559  \n",
    "BAMS_V 506 BA1 BA2 2024W1 Optimal Decision Making I = 148028\n",
    "BAIT_V 508 BA1 BA2 2024W1 Business Analytics Programming = 147623  \n",
    "Learning Analytics hackathon =161721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import dotenv\n",
    "import os\n",
    "import canvasapi\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "\n",
    "TOKEN = os.environ.get('CANVAS_API_TOKEN')\n",
    "BASEURL = 'https://ubc.instructure.com'\n",
    "\n",
    "canvas_api = canvasapi.Canvas(BASEURL, TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Course information using Canvas API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_ids = [161721, 147283, 145559, 148028, 147623]  # Replace with your actual course IDs\n",
    "\n",
    "# Initialize a list to collect course data\n",
    "course_data = []\n",
    "\n",
    "# Iterate through each course ID\n",
    "for course_id in course_ids:\n",
    "    course = canvas_api.get_course(course_id)\n",
    "    \n",
    "    # Course Details\n",
    "    course_name = course.name\n",
    "    course_start_date = course.start_at\n",
    "    course_end_date = course.end_at\n",
    "\n",
    "    # Formatting dates\n",
    "    if course_start_date:\n",
    "        course_start_date = datetime.strptime(course_start_date, \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%d\")\n",
    "    if course_end_date:\n",
    "        course_end_date = datetime.strptime(course_end_date, \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Get all students in the course\n",
    "    students = course.get_users(enrollment_type=['student'])\n",
    "\n",
    "    # Count the total number of students\n",
    "    student_count = len(list(students))\n",
    "\n",
    "    # Append course information to the list\n",
    "    course_data.append({\n",
    "        'Course ID': course_id,\n",
    "        'Course Title': course_name,\n",
    "        'Course Start Date': course_start_date if course_start_date else 'N/A',\n",
    "        'Course End Date': course_end_date if course_end_date else 'N/A',\n",
    "        'Total Number of Registered Students': student_count\n",
    "    })\n",
    "\n",
    "# Create a DataFrame from the collected course data\n",
    "course_df = pd.DataFrame(course_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(course_df)\n",
    "course_df.to_csv('Course_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Discussion information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discussion_data = []\n",
    "\n",
    "# Iterate through each course ID\n",
    "for course_id in course_ids:\n",
    "    course = canvas_api.get_course(course_id)\n",
    "    \n",
    "    # Fetch course details\n",
    "    course_name = course.name\n",
    "    \n",
    "    # Fetch discussions in the course\n",
    "    discussions = course.get_discussion_topics()\n",
    "\n",
    "    # Get all users in the course to map their IDs to enrollment types\n",
    "    users = {user.id: user for user in course.get_users()}\n",
    "    \n",
    "    # Get enrollment info for each user in the course\n",
    "    enrollments = {enrollment.user_id: enrollment.type for enrollment in course.get_enrollments()}\n",
    "\n",
    "    for discussion in discussions:\n",
    "        discussion_id = discussion.id\n",
    "        discussion_title = discussion.title\n",
    "        discussion_content = discussion.message\n",
    "        soup = BeautifulSoup(discussion_content, \"html.parser\")\n",
    "        discussion_content = soup.get_text()\n",
    "        # Fetch entries related to the discussion and convert to list\n",
    "        entries = list(discussion.get_topic_entries())\n",
    "\n",
    "        # Count replies to the discussion (total entries minus the main discussion entry)\n",
    "        reply_count = len(entries) if entries else 0  # Ensure reply count is 0 if there are no entries\n",
    "\n",
    "        # Iterate over each entry to fetch user ID and role\n",
    "        for entry in entries:\n",
    "            entry_id = entry.id\n",
    "            entry_content= entry.message\n",
    "            soup = BeautifulSoup(entry_content, \"html.parser\")\n",
    "            entry_content = soup.get_text()\n",
    "            \n",
    "            entry_user_id = entry.user_id  # Get the user ID of the entry owner\n",
    "            \n",
    "            # Get the enrollment type (role) based on the user ID\n",
    "            user_role = enrollments.get(entry_user_id, None)  # Default to None if not found\n",
    "            replies = list(entry.get_replies())  # Get replies for the entry\n",
    "            entry_reply_count = len(replies)\n",
    "            print(\"Entry replies\",reply_count)\n",
    "            \n",
    "            combined_message = f\"{discussion_title}\\n{entry_content}\"  # Start with the entry message\n",
    "\n",
    "            for reply in replies:\n",
    "                reply_message=reply.message\n",
    "                soup = BeautifulSoup(reply_message, \"html.parser\")\n",
    "                reply_message = soup.get_text()\n",
    "                combined_message += f\"\\n{reply_message}\"\n",
    "            print(\"Combined message\",combined_message)\n",
    "            \n",
    "            # Append the discussion information for each entry to the list\n",
    "            discussion_data.append({\n",
    "                'Course ID': course_id,\n",
    "                'Course Name': course_name,\n",
    "                'Discussion ID': discussion_id,\n",
    "                'Discussion Title': discussion_title,\n",
    "                'Discussion Content': discussion_content,\n",
    "                'Number of Total Entries': reply_count,\n",
    "                'Entry ID': entry_id,\n",
    "                'Entry Content':   entry_content,     \n",
    "                'User ID of Entry Owner': entry_user_id,\n",
    "                'Role of Entry Owner': user_role,\n",
    "                'Number of Entry replies': entry_reply_count,\n",
    "                'Combined Message': combined_message\n",
    "            })\n",
    "\n",
    "# Create a DataFrame from the collected discussion data\n",
    "discussion_df = pd.DataFrame(discussion_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(discussion_df)\n",
    "discussion_df.to_csv('final_entries_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Discussion ID to aggregate the results\n",
    "final_df = discussion_df.groupby(\n",
    "    ['Course ID', 'Course Name', 'Discussion ID', 'Discussion Title', 'Discussion Content'],\n",
    "    as_index=False\n",
    ").agg(\n",
    "    Number_of_Entry_Replies=('Number of Entry replies', 'sum'),\n",
    "    Combined_Message=('Combined Message', ' '.join),  # Concatenate messages\n",
    "    Max_Depth_of_Entry=('Number of Entry replies', 'max')  # Get max replies for each entry\n",
    ")\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(final_df)\n",
    "\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "sw = stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    ''' This function takes a string as input and \n",
    "        returns a cleaned version of the string\n",
    "        Specifically, it makes the string into lower case and remove punctuations\n",
    "    '''\n",
    "    text_lower = text.lower() # make it lowercase\n",
    "    text_no_punctuation = text_lower.translate(translator) # remove punctuation  \n",
    "    clean_words = [w for w in text_no_punctuation.split() if w not in sw] # remove stopwords\n",
    "    return ' '.join(clean_words)\n",
    "final_df['Cleaned_combined'] = final_df['Combined_Message'].apply(clean_text)\n",
    "final_df.to_csv('Cleaned_final_entries_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "course_df = pd.read_csv(\"course_keywords_entries_data.csv\", encoding = 'utf-8')\n",
    "\n",
    "def get_keywords_tfidf(document_list):\n",
    "    '''\n",
    "    This function gets a list of documents as input and returns a list of top 10 keywords for each document using TF-IDF scores.\n",
    "    Input: A list of documents (text)\n",
    "    Output: The corresponding top 10 keywords for each document based on tf-idf values\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer() # Step 1: Create a TF-IDF vectorizer\n",
    "    tfidf_matrix = vectorizer.fit_transform(document_list) # Step 2: Calculate the TF-IDF matrix\n",
    "    feature_names = vectorizer.get_feature_names_out() # Step 3: Get feature names (words)\n",
    "\n",
    "    # Step 4: Extract top 10 keywords for each document\n",
    "    top_keywords = [] # accumulator\n",
    "    for i in range(len(document_list)):\n",
    "        feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
    "        #print(\"feature index\",feature_index)\n",
    "        feature_value = [tfidf_matrix[i, x] for x in feature_index]\n",
    "        #print(\"Feature value\",feature_value)\n",
    "        tfidf_scores = zip(feature_index, feature_value)\n",
    "        sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "        #print(sorted_tfidf_scores)\n",
    "        top_keywords.append(' '.join([feature_names[i] for i, _ in sorted_tfidf_scores[:10]]))\n",
    "\n",
    "        if i % 200 == 199:\n",
    "            print(f'Processed {i+1}/{len(document_list)} documents.')\n",
    "    \n",
    "    return top_keywords\n",
    "\n",
    "course_df[\"course_keywords\"]=get_keywords_tfidf(course_df['Course_message'].tolist())\n",
    "print(course_df[\"course_keywords\"])\n",
    "#course_df.to_csv('course_keywords_final_entries_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis- Semantic Similarity using Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"keywords_data.csv\", encoding='utf-8')\n",
    "docs = [row.split() for row in df['Cleaned_combined']]\n",
    "\n",
    "# training word2vec model using the list of words in docs\n",
    "model = Word2Vec(docs, min_count=5, vector_size=50, workers=3, window=5, sg = 1)\n",
    "# save the model for future use; you don't need to train Word2Vec for multiple times\n",
    "model.save(\"word2vec_proj.model\")\n",
    "# load model from stored file\n",
    "model = Word2Vec.load(\"word2vec_proj.model\")\n",
    "\n",
    "#new_entry= \"Fries better with ketchup than with mustard\"\n",
    "## Relevant keywords---> services/ technology / Business / engineering\n",
    "words_to_check = [\"mustard\", \"ketchup\", \"fries\"]\n",
    "results = []\n",
    "# Find top 5 similar words for each specified word\n",
    "for word in words_to_check:\n",
    "    top_similar_words = model.wv.most_similar(word, topn=2)  # Getting the top 5 similar words\n",
    "    print(f\"Top 5 words similar to {word}:\")\n",
    "    for similar_word, score in top_similar_words:\n",
    "        print(f\"{similar_word}: {score:.4f}\")\n",
    "        results.append({\n",
    "            'Input Word': word,\n",
    "            'Similar Word': similar_word,\n",
    "            'Similarity Score': score\n",
    "        })\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "similarity_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(similarity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis- Wordcloud (Discussion within the Course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(keywords, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, max_font_size=100, background_color='white').generate(' '.join(keywords))\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "df=df[df['Course ID']==161721]\n",
    "print(df)\n",
    "# Generate a word cloud for each discussion\n",
    "for index, row in df.iterrows():\n",
    "    plot_wordcloud(row['top_keyword_tfidf'].split(), row['Discussion Title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis- Network Graph (Across Courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv(\"keywords_data.csv\", encoding='utf-8')\n",
    "\n",
    "# Step 2: Compute TF-IDF and Cosine Similarity\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Cleaned_combined'])\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Step 3: Create a NetworkX graph\n",
    "threshold = 0.2  # Set a threshold for similarity to consider an edge\n",
    "G = nx.Graph()\n",
    "\n",
    "# Create a color map for each course ID\n",
    "course_ids = df['Course ID'].unique()  # Get unique course IDs\n",
    "colors = plt.cm.get_cmap('tab10', len(course_ids))  # Create a colormap\n",
    "course_color_map = {course_id: colors(i) for i, course_id in enumerate(course_ids)}  # Map course IDs to colors\n",
    "\n",
    "# Add nodes and edges based on the similarity matrix\n",
    "for i in range(similarity_matrix.shape[0]):\n",
    "    course_id = df['Course ID'].iloc[i]  # Get the course ID for the current discussion\n",
    "    discussion_title = df['Discussion Title'].iloc[i]  # Get the discussion title\n",
    "    G.add_node(i, course_id=course_id, title=discussion_title)  # Store course ID and title as node attributes\n",
    "    for j in range(i + 1, similarity_matrix.shape[1]):\n",
    "        if similarity_matrix[i][j] > threshold:  # Check if similarity exceeds threshold\n",
    "            G.add_edge(i, j, weight=similarity_matrix[i][j])\n",
    "\n",
    "# Step 4: Draw the network diagram\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G)  # Use a layout for better visualization\n",
    "\n",
    "# Assign colors based on course ID\n",
    "node_colors = [course_color_map[G.nodes[i]['course_id']] for i in G.nodes]\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_size=700, node_color=node_colors)\n",
    "nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n",
    "\n",
    "# Add labels with small font size\n",
    "labels = {i: G.nodes[i]['title'] for i in G.nodes}  # Create a dictionary for labels\n",
    "nx.draw_networkx_labels(G, pos, labels, font_size=8)  # Adjust font_size for smaller labels\n",
    "\n",
    "# Create a legend for course colors\n",
    "legend_handles = [mpatches.Patch(color=course_color_map[course_id], label=f'Course ID: {course_id}') for course_id in course_ids]\n",
    "\n",
    "# Add legend to the plot\n",
    "plt.legend(handles=legend_handles, title=\"Courses\", loc='upper right')\n",
    "\n",
    "plt.title(\"Discussion Similarity Network\")\n",
    "plt.axis('off')  # Hide the axes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
